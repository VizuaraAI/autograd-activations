<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Why Does Autograd Save Activations?</title>
<meta name="description" content="A visual explainer on why PyTorch autograd saves activations for backpropagation instead of pre-computing gradients during the forward pass.">
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600&family=Source+Serif+4:ital,wght@0,400;0,700;1,400&family=Nunito+Sans:wght@300;400;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --bg: #0b0d11;
  --bg2: #12151c;
  --card: #171b24;
  --border: #232836;
  --text: #d8dce6;
  --dim: #7c8294;
  --muted: #4e5468;
  --blue: #6a9ffa;
  --cyan: #50d8c0;
  --orange: #f0945a;
  --purple: #a78bfa;
  --red: #f06a7a;
  --yellow: #f0d060;
  --green: #60e890;
}

* { margin: 0; padding: 0; box-sizing: border-box; }
html { scroll-behavior: smooth; }
body {
  font-family: 'Nunito Sans', sans-serif;
  background: var(--bg);
  color: var(--text);
  line-height: 1.75;
  -webkit-font-smoothing: antialiased;
}
::selection { background: var(--blue); color: #fff; }

.wrap { max-width: 780px; margin: 0 auto; padding: 0 1.5rem; }

/* ‚îÄ‚îÄ Header ‚îÄ‚îÄ */
header {
  padding: 5rem 0 3rem;
  text-align: center;
  position: relative;
}
header::before {
  content: '';
  position: absolute;
  inset: 0;
  background: radial-gradient(ellipse 50% 60% at 50% 30%, rgba(106,159,250,0.07), transparent);
  pointer-events: none;
}
.tag {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.68rem;
  letter-spacing: 0.2em;
  text-transform: uppercase;
  color: var(--cyan);
  margin-bottom: 1rem;
}
h1 {
  font-family: 'Source Serif 4', serif;
  font-size: clamp(2rem, 5vw, 3.2rem);
  font-weight: 700;
  line-height: 1.2;
  margin-bottom: 1rem;
}
h1 em { font-style: italic; color: var(--orange); }
header p {
  color: var(--dim);
  font-size: 1.05rem;
  max-width: 560px;
  margin: 0 auto;
}

/* ‚îÄ‚îÄ Sections ‚îÄ‚îÄ */
section { padding: 3rem 0; }
.divider {
  width: 40px; height: 2px;
  background: var(--border);
  margin: 0 auto;
}

.sec-label {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.65rem;
  letter-spacing: 0.18em;
  text-transform: uppercase;
  color: var(--purple);
  margin-bottom: 0.5rem;
}
h2 {
  font-family: 'Source Serif 4', serif;
  font-size: 1.7rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.lead { color: var(--dim); margin-bottom: 2rem; }

/* ‚îÄ‚îÄ Cards ‚îÄ‚îÄ */
.card {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 1.5rem 1.8rem;
  margin-bottom: 1rem;
  transition: border-color 0.2s;
}
.card:hover { border-color: #333a4a; }

.wrong {
  border-left: 3px solid var(--red);
  background: linear-gradient(135deg, rgba(240,106,122,0.04), transparent);
}
.right {
  border-left: 3px solid var(--cyan);
  background: linear-gradient(135deg, rgba(80,216,192,0.04), transparent);
}
.insight {
  border-left: 3px solid var(--yellow);
  background: linear-gradient(135deg, rgba(240,208,96,0.04), transparent);
}

.card-label {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.65rem;
  letter-spacing: 0.15em;
  text-transform: uppercase;
  font-weight: 600;
  margin-bottom: 0.5rem;
}
.wrong .card-label { color: var(--red); }
.right .card-label { color: var(--cyan); }
.insight .card-label { color: var(--yellow); }

.card p { color: var(--dim); font-size: 0.92rem; }
.card code {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.82rem;
  background: rgba(106,159,250,0.1);
  color: var(--blue);
  padding: 0.1rem 0.35rem;
  border-radius: 4px;
}

/* ‚îÄ‚îÄ Steps ‚îÄ‚îÄ */
.steps { margin: 1.5rem 0; }
.step {
  display: grid;
  grid-template-columns: 40px 1fr;
  gap: 1rem;
  margin-bottom: 1.5rem;
  position: relative;
}
.step::before {
  content: '';
  position: absolute;
  left: 19px;
  top: 36px;
  bottom: -12px;
  width: 2px;
  background: var(--border);
}
.step:last-child::before { display: none; }

.dot {
  width: 40px; height: 40px;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.75rem;
  font-weight: 600;
  color: #fff;
  flex-shrink: 0;
}
.dot.fwd { background: var(--blue); }
.dot.save { background: var(--cyan); }
.dot.bwd { background: var(--orange); }

.step-card {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 10px;
  padding: 1.2rem 1.4rem;
}
.step-card h4 { font-size: 1rem; margin-bottom: 0.3rem; }
.step-card p { font-size: 0.88rem; color: var(--dim); }

.code {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.78rem;
  background: var(--bg);
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 0.8rem 1rem;
  margin-top: 0.8rem;
  color: var(--dim);
  line-height: 1.7;
  overflow-x: auto;
  white-space: pre;
}
.code .kw { color: var(--purple); }
.code .fn { color: var(--blue); }
.code .cm { color: var(--muted); }
.code .v { color: var(--cyan); }
.code .op { color: var(--orange); }

/* ‚îÄ‚îÄ Proof grid ‚îÄ‚îÄ */
.proof-grid {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(340px, 1fr));
  gap: 1rem;
  margin: 1.5rem 0;
}
.proof {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 10px;
  padding: 1.3rem 1.5rem;
}
.proof .op {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.62rem;
  letter-spacing: 0.15em;
  text-transform: uppercase;
  color: var(--purple);
  margin-bottom: 0.3rem;
}
.proof h4 {
  font-family: 'Source Serif 4', serif;
  font-size: 1.15rem;
  margin-bottom: 0.8rem;
}
.math {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.82rem;
  padding: 0.5rem 0.8rem;
  background: var(--bg);
  border-radius: 6px;
  margin-bottom: 0.4rem;
  color: var(--dim);
  border-left: 3px solid var(--border);
}
.math.needs { border-left-color: var(--orange); }
.math .n { color: var(--orange); font-weight: 600; }
.proof .note {
  font-size: 0.8rem;
  color: var(--muted);
  margin-top: 0.6rem;
}

/* ‚îÄ‚îÄ Tensor anatomy ‚îÄ‚îÄ */
.anatomy {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(220px, 1fr));
  gap: 0.8rem;
  margin: 1.5rem 0;
}
.anat-card {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 10px;
  padding: 1.2rem;
}
.anat-card .ico { font-size: 1.3rem; margin-bottom: 0.6rem; }
.anat-card h5 {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.85rem;
  margin-bottom: 0.3rem;
}
.anat-card p { font-size: 0.8rem; color: var(--muted); }

/* ‚îÄ‚îÄ Memory bars ‚îÄ‚îÄ */
.mem-section {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 1rem;
  margin: 1.5rem 0;
}
@media (max-width: 600px) { .mem-section { grid-template-columns: 1fr; } }
.mem-col {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 10px;
  padding: 1.3rem;
}
.mem-col h4 {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.68rem;
  letter-spacing: 0.12em;
  text-transform: uppercase;
  margin-bottom: 1rem;
}
.bar-row { margin-bottom: 0.7rem; }
.bar-label {
  display: flex;
  justify-content: space-between;
  font-size: 0.78rem;
  color: var(--dim);
  margin-bottom: 0.2rem;
  font-family: 'JetBrains Mono', monospace;
}
.bar { height: 6px; background: var(--bg); border-radius: 3px; overflow: hidden; }
.bar-fill { height: 100%; border-radius: 3px; }
.mem-col .note { font-size: 0.78rem; color: var(--muted); margin-top: 0.8rem; }

/* ‚îÄ‚îÄ Interactive sim ‚îÄ‚îÄ */
.sim {
  background: var(--bg2);
  border: 1px solid var(--border);
  border-radius: 12px;
  overflow: hidden;
  margin: 1.5rem 0;
}
.sim-bar {
  display: flex;
  align-items: center;
  gap: 0.4rem;
  padding: 0.8rem 1rem;
  border-bottom: 1px solid var(--border);
  flex-wrap: wrap;
}
.sim-bar h4 {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.68rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--dim);
  margin-right: auto;
}
.sbtn {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.7rem;
  padding: 0.4rem 0.8rem;
  background: var(--card);
  color: var(--dim);
  border: 1px solid var(--border);
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s;
}
.sbtn:hover { color: var(--text); border-color: #444; }
.sbtn.on { background: var(--blue); color: #fff; border-color: var(--blue); }

.sim-body {
  display: grid;
  grid-template-columns: 1fr 1fr;
  min-height: 380px;
}
@media (max-width: 700px) { .sim-body { grid-template-columns: 1fr; } }

.sim-left {
  padding: 1.2rem;
  border-right: 1px solid var(--border);
  display: flex;
  align-items: center;
  justify-content: center;
}
.sim-right {
  padding: 1.2rem;
  font-size: 0.85rem;
  overflow-y: auto;
  max-height: 380px;
}
.sim-right h5 {
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.62rem;
  letter-spacing: 0.12em;
  text-transform: uppercase;
  color: var(--muted);
  margin: 1rem 0 0.4rem;
}
.sim-right h5:first-child { margin-top: 0; }

.sr {
  display: flex;
  align-items: center;
  gap: 0.6rem;
  padding: 0.35rem 0.5rem;
  border-radius: 4px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 0.75rem;
  margin-bottom: 0.2rem;
}
.sr .nm { color: var(--text); min-width: 70px; }
.sr .vl { color: var(--cyan); }
.sr .vl.gv { color: var(--orange); }
.sr.sv { background: rgba(80,216,192,0.05); }
.sr.gd { background: rgba(240,148,90,0.05); }
.sr .tg {
  font-size: 0.55rem;
  padding: 0.1rem 0.3rem;
  border-radius: 3px;
  letter-spacing: 0.05em;
  text-transform: uppercase;
  margin-left: auto;
  flex-shrink: 0;
}
.tg.s { background: rgba(80,216,192,0.12); color: var(--cyan); }
.tg.g { background: rgba(240,148,90,0.12); color: var(--orange); }
.tg.p { background: rgba(167,139,250,0.12); color: var(--purple); }

.sim-note {
  font-size: 0.78rem;
  color: var(--muted);
  margin-top: 0.4rem;
  font-family: 'JetBrains Mono', monospace;
  line-height: 1.5;
}
.sim-note.hl { color: var(--cyan); }
.sim-note.rd { color: var(--red); font-weight: 600; }

/* ‚îÄ‚îÄ Footer ‚îÄ‚îÄ */
footer {
  text-align: center;
  padding: 3rem 1.5rem;
  color: var(--muted);
  font-size: 0.75rem;
  border-top: 1px solid var(--border);
  margin-top: 2rem;
}

/* ‚îÄ‚îÄ Animations ‚îÄ‚îÄ */
.reveal {
  opacity: 0;
  transform: translateY(20px);
  transition: all 0.6s cubic-bezier(0.22,1,0.36,1);
}
.reveal.vis { opacity: 1; transform: translateY(0); }

@keyframes dash { to { stroke-dashoffset: -20; } }
.flowing { animation: dash 0.8s linear infinite; }
</style>
</head>
<body>

<header>
  <div class="wrap">
    <div class="tag">Autograd ¬∑ Backpropagation ¬∑ PyTorch Internals</div>
    <h1>Why Does Autograd Save <em>Activations</em> for Backward?</h1>
    <p>Autograd doesn't pre-compute gradients during the forward pass. It saves the <strong>ingredients</strong> each operation will need to compute gradients later. Here's exactly why.</p>
  </div>
</header>

<!-- ‚ïê‚ïê‚ïê 1. THE MISCONCEPTION ‚ïê‚ïê‚ïê -->
<section>
  <div class="wrap">
    <div class="sec-label reveal">01 ‚Äî The Misconception</div>
    <h2 class="reveal">What people think vs. what actually happens</h2>

    <div class="card wrong reveal">
      <div class="card-label">‚úó Wrong Mental Model</div>
      <p>"During the forward pass, autograd computes all the gradients and stores them with each Tensor. Then <code>.backward()</code> just topologically sorts these pre-computed gradients and chains them."</p>
    </div>

    <div class="card right reveal">
      <div class="card-label">‚úì What Actually Happens</div>
      <p>During forward, autograd builds a <strong>DAG of operations</strong>. For each op, it records a <code>grad_fn</code> ‚Äî a <em>recipe</em> for computing the gradient ‚Äî and saves the <strong>input activations</strong> the recipe will need. No gradient value is computed until <code>.backward()</code> is called.</p>
    </div>

    <div class="card insight reveal">
      <div class="card-label">üí° Why the confusion?</div>
      <p>"Autograd" suggests automatic gradient computation. It's natural to assume gradients are computed automatically during forward. In reality, it means the system automatically <strong>records</strong> what it needs during forward so it can <strong>defer</strong> gradient computation to backward. This is called <em>reverse-mode automatic differentiation</em>.</p>
    </div>
  </div>
</section>

<div class="divider"></div>

<!-- ‚ïê‚ïê‚ïê 2. STEP BY STEP ‚ïê‚ïê‚ïê -->
<section>
  <div class="wrap">
    <div class="sec-label reveal">02 ‚Äî Step by Step</div>
    <h2 class="reveal">Following <code>y = relu(x @ W + b)</code> through autograd</h2>
    <p class="lead reveal">At each operation, watch what gets created and what gets saved.</p>

    <div class="steps">
      <div class="step reveal">
        <div class="dot fwd">1</div>
        <div class="step-card">
          <h4>MatMul: z‚ÇÅ = x @ W</h4>
          <p>Creates a <code>MmBackward</code> node in the graph. <strong>Saves x and W</strong> because the gradient formulas need them.</p>
          <div class="code"><span class="v">z1</span> = <span class="v">x</span> <span class="op">@</span> <span class="v">W</span>
<span class="cm"># grad_fn = MmBackward</span>
<span class="cm"># saved:   x (for ‚àÇL/‚àÇW = x·µÄ @ grad)</span>
<span class="cm">#          W (for ‚àÇL/‚àÇx = grad @ W·µÄ)</span></div>
        </div>
      </div>

      <div class="step reveal">
        <div class="dot fwd">2</div>
        <div class="step-card">
          <h4>Add: z‚ÇÇ = z‚ÇÅ + b</h4>
          <p>Creates <code>AddBackward</code>. Addition's gradient is just 1 ‚Äî <strong>nothing needs to be saved</strong>.</p>
          <div class="code"><span class="v">z2</span> = <span class="v">z1</span> <span class="op">+</span> <span class="v">b</span>
<span class="cm"># grad_fn = AddBackward</span>
<span class="cm"># saved:   nothing (grad = pass-through)</span></div>
        </div>
      </div>

      <div class="step reveal">
        <div class="dot fwd">3</div>
        <div class="step-card">
          <h4>ReLU: y = relu(z‚ÇÇ)</h4>
          <p>Creates <code>ReluBackward</code>. <strong>Saves z‚ÇÇ</strong> (or the boolean mask <code>z‚ÇÇ > 0</code>) because ReLU's gradient is 1 where positive, 0 elsewhere.</p>
          <div class="code"><span class="v">y</span> = <span class="v">z2</span>.<span class="fn">relu</span>()
<span class="cm"># grad_fn = ReluBackward</span>
<span class="cm"># saved:   z2 ‚Üí mask = (z2 > 0)</span></div>
        </div>
      </div>

      <div class="step reveal">
        <div class="dot bwd">‚Üê</div>
        <div class="step-card">
          <h4>loss.backward() ‚Äî NOW gradients are computed</h4>
          <p>The engine toposorts the DAG and walks it in reverse. At each node, it runs the <code>grad_fn</code> using <strong>saved tensors + incoming upstream gradient</strong> to produce the gradient for that op.</p>
          <div class="code"><span class="v">loss</span>.<span class="fn">backward</span>()
<span class="cm"># Engine walks graph in reverse:</span>
<span class="cm"># ReluBackward: grad * (z2 > 0)       ‚Üê uses saved z2</span>
<span class="cm"># AddBackward:  pass grad through</span>
<span class="cm"># MmBackward:   ‚àÇW = x·µÄ @ grad        ‚Üê uses saved x</span>
<span class="cm">#               ‚àÇx = grad @ W·µÄ        ‚Üê uses saved W</span></div>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="divider"></div>

<!-- ‚ïê‚ïê‚ïê 3. MATHEMATICAL PROOF ‚ïê‚ïê‚ïê -->
<section>
  <div class="wrap">
    <div class="sec-label reveal">03 ‚Äî The Math</div>
    <h2 class="reveal">Each backward formula needs forward values</h2>
    <p class="lead reveal">The <span style="color:var(--orange)">orange</span> terms must come from saved forward state. This is why activations are stored.</p>

    <div class="proof-grid">
      <div class="proof reveal">
        <div class="op">Matrix Multiply</div>
        <h4>Y = X @ W</h4>
        <div class="math needs">‚àÇL/‚àÇW = <span class="n">X·µÄ</span> @ ‚àÇL/‚àÇY</div>
        <div class="math needs">‚àÇL/‚àÇX = ‚àÇL/‚àÇY @ <span class="n">W·µÄ</span></div>
        <p class="note">Need saved X and W.</p>
      </div>

      <div class="proof reveal">
        <div class="op">ReLU</div>
        <h4>Y = max(0, X)</h4>
        <div class="math needs">‚àÇL/‚àÇX = ‚àÇL/‚àÇY ‚äô (<span class="n">X</span> > 0)</div>
        <p class="note">Need saved X (or its sign mask).</p>
      </div>

      <div class="proof reveal">
        <div class="op">Softmax</div>
        <h4>Y·µ¢ = exp(X·µ¢) / Œ£exp(X‚±º)</h4>
        <div class="math needs">‚àÇL/‚àÇX = <span class="n">Y</span> ‚äô (‚àÇL/‚àÇY ‚àí ‚àÇL/‚àÇY ¬∑ <span class="n">Y</span>)</div>
        <p class="note">Need saved output Y (the probabilities).</p>
      </div>

      <div class="proof reveal">
        <div class="op">Element-wise Multiply</div>
        <h4>Y = A ‚äô B</h4>
        <div class="math needs">‚àÇL/‚àÇA = ‚àÇL/‚àÇY ‚äô <span class="n">B</span></div>
        <div class="math needs">‚àÇL/‚àÇB = ‚àÇL/‚àÇY ‚äô <span class="n">A</span></div>
        <p class="note">Each input is the other's gradient coefficient.</p>
      </div>

      <div class="proof reveal">
        <div class="op">Addition (the exception)</div>
        <h4>Y = A + B</h4>
        <div class="math">‚àÇL/‚àÇA = ‚àÇL/‚àÇY</div>
        <div class="math">‚àÇL/‚àÇB = ‚àÇL/‚àÇY</div>
        <p class="note">Gradient is 1. No saved values needed ‚Äî the rare exception.</p>
      </div>

      <div class="proof reveal">
        <div class="op">Layer Normalization</div>
        <h4>Y = (X ‚àí Œº) / œÉ</h4>
        <div class="math needs">‚àÇL/‚àÇX = f(‚àÇL/‚àÇY, <span class="n">X</span>, <span class="n">Œº</span>, <span class="n">œÉ</span>)</div>
        <p class="note">Need saved input, mean, and std deviation.</p>
      </div>
    </div>

    <div class="card insight reveal">
      <div class="card-label">üí° The Pattern</div>
      <p>For almost every operation, the backward formula needs <strong>two things</strong>: (1) the upstream gradient flowing from the next layer, and (2) some value saved during forward ‚Äî an input, output, or intermediate. Autograd saves <em>only</em> what each operation specifically needs.</p>
    </div>
  </div>
</section>

<div class="divider"></div>

<!-- ‚ïê‚ïê‚ïê 4. INTERACTIVE SIMULATION ‚ïê‚ïê‚ïê -->
<section>
  <div class="wrap">
    <div class="sec-label reveal">04 ‚Äî Interactive</div>
    <h2 class="reveal">Watch the autograd engine step by step</h2>
    <p class="lead reveal">Click each step. Notice how <code>.grad</code> stays None until backward.</p>

    <div class="sim reveal">
      <div class="sim-bar">
        <h4>Autograd Simulation</h4>
        <button class="sbtn on" data-s="0">Reset</button>
        <button class="sbtn" data-s="1">x @ W</button>
        <button class="sbtn" data-s="2">+ b</button>
        <button class="sbtn" data-s="3">relu</button>
        <button class="sbtn" data-s="4">loss</button>
        <button class="sbtn" data-s="5">backward()</button>
      </div>
      <div class="sim-body">
        <div class="sim-left">
          <svg id="sg" viewBox="0 0 400 350" width="100%" height="100%"></svg>
        </div>
        <div class="sim-right" id="sp">
          <h5>State</h5>
          <p style="color:var(--muted);font-size:0.82rem;">Click a step to walk through the forward pass ‚Üí</p>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="divider"></div>

<!-- ‚ïê‚ïê‚ïê 5. TENSOR ANATOMY ‚ïê‚ïê‚ïê -->
<section>
  <div class="wrap">
    <div class="sec-label reveal">05 ‚Äî PyTorch Internals</div>
    <h2 class="reveal">What's inside a Tensor with <code>requires_grad=True</code></h2>

    <div class="anatomy">
      <div class="anat-card reveal">
        <div class="ico">üì¶</div>
        <h5>.data</h5>
        <p>The actual numeric values stored in memory.</p>
      </div>
      <div class="anat-card reveal">
        <div class="ico">üîó</div>
        <h5>.grad_fn</h5>
        <p>Pointer to the backward function that created this tensor. The graph node.</p>
      </div>
      <div class="anat-card reveal">
        <div class="ico">üíæ</div>
        <h5>.grad_fn.saved_tensors</h5>
        <p>The activations saved during forward. <strong>This is where they live.</strong></p>
      </div>
      <div class="anat-card reveal">
        <div class="ico">üìä</div>
        <h5>.grad</h5>
        <p>Initially <strong>None</strong>. Only filled <em>after</em> <code>.backward()</code> runs.</p>
      </div>
      <div class="anat-card reveal">
        <div class="ico">üè∑Ô∏è</div>
        <h5>.requires_grad</h5>
        <p>Boolean flag that tells autograd to track operations on this tensor.</p>
      </div>
      <div class="anat-card reveal">
        <div class="ico">üîô</div>
        <h5>.next_functions</h5>
        <p>DAG edges linking to parent grad_fns. This is what gets topologically sorted.</p>
      </div>
    </div>

    <div class="card insight reveal">
      <div class="card-label">üí° The Key Distinction</div>
      <p>A common misconception is that gradients for all ops are calculated during forward. In reality: <code>.grad_fn</code> is <em>registered</em> during forward (a function pointer, not a computed value). <code>.grad</code> stays <code>None</code> until backward. The actual gradient values only exist after <code>.backward()</code> runs, because each gradient depends on the <strong>upstream gradient</strong> from the next layer ‚Äî which doesn't exist yet during forward.</p>
    </div>
  </div>
</section>

<div class="divider"></div>

<!-- ‚ïê‚ïê‚ïê 6. MEMORY COST ‚ïê‚ïê‚ïê -->
<section>
  <div class="wrap">
    <div class="sec-label reveal">06 ‚Äî Memory Cost</div>
    <h2 class="reveal">Activation memory is the training bottleneck</h2>
    <p class="lead reveal">If gradients were pre-computed in forward, there'd be no memory problem. The fact that saved activations dominate GPU memory proves they're needed.</p>

    <div class="mem-section reveal">
      <div class="mem-col">
        <h4 style="color:var(--cyan)">Standard Training</h4>
        <div class="bar-row">
          <div class="bar-label"><span>Parameters</span><span style="color:var(--purple)">2 GB</span></div>
          <div class="bar"><div class="bar-fill" style="width:15%;background:var(--purple)"></div></div>
        </div>
        <div class="bar-row">
          <div class="bar-label"><span>Gradients</span><span style="color:var(--orange)">2 GB</span></div>
          <div class="bar"><div class="bar-fill" style="width:15%;background:var(--orange)"></div></div>
        </div>
        <div class="bar-row">
          <div class="bar-label"><span>Optimizer</span><span style="color:var(--blue)">4 GB</span></div>
          <div class="bar"><div class="bar-fill" style="width:30%;background:var(--blue)"></div></div>
        </div>
        <div class="bar-row">
          <div class="bar-label"><span>Activations</span><span style="color:var(--cyan)">8+ GB</span></div>
          <div class="bar"><div class="bar-fill" style="width:80%;background:var(--cyan)"></div></div>
        </div>
        <p class="note">Activations dominate. For a 1B model at batch 32, they can be 10√ó the model size.</p>
      </div>

      <div class="mem-col">
        <h4 style="color:var(--red)">Gradient Checkpointing</h4>
        <div class="bar-row">
          <div class="bar-label"><span>Parameters</span><span style="color:var(--purple)">2 GB</span></div>
          <div class="bar"><div class="bar-fill" style="width:15%;background:var(--purple)"></div></div>
        </div>
        <div class="bar-row">
          <div class="bar-label"><span>Gradients</span><span style="color:var(--orange)">2 GB</span></div>
          <div class="bar"><div class="bar-fill" style="width:15%;background:var(--orange)"></div></div>
        </div>
        <div class="bar-row">
          <div class="bar-label"><span>Optimizer</span><span style="color:var(--blue)">4 GB</span></div>
          <div class="bar"><div class="bar-fill" style="width:30%;background:var(--blue)"></div></div>
        </div>
        <div class="bar-row">
          <div class="bar-label"><span>Activations</span><span style="color:var(--red)">~1 GB</span></div>
          <div class="bar"><div class="bar-fill" style="width:8%;background:var(--red)"></div></div>
        </div>
        <p class="note">Only save at checkpoints. Re-run forward during backward. ~30% more compute, far less memory.</p>
      </div>
    </div>

    <div class="card insight reveal">
      <div class="card-label">üí° The Ultimate Proof</div>
      <p>If autograd pre-computed gradients during forward, there would be <strong>no memory problem</strong> ‚Äî just store gradients (same size as parameters) and discard everything. The fact that GPU memory during training is overwhelmingly consumed by saved activations ‚Äî and that gradient checkpointing exists to address this ‚Äî proves gradients are <strong>not</strong> computed during forward.</p>
    </div>
  </div>
</section>

<footer>
  <p>Autograd ¬∑ Activations ¬∑ Backpropagation ‚Äî A Visual Explainer</p>
</footer>

<script>
// Scroll reveal
const obs = new IntersectionObserver(es => {
  es.forEach(e => { if (e.isIntersecting) e.target.classList.add('vis'); });
}, { threshold: 0.08 });
document.querySelectorAll('.reveal').forEach(el => obs.observe(el));

// ‚îÄ‚îÄ‚îÄ Simulation ‚îÄ‚îÄ‚îÄ
const sg = document.getElementById('sg');
const sp = document.getElementById('sp');

const N = [
  {id:'x',  l:'x',     s:'input',  x:30, y:160,w:70,h:34,c:'#6a9ffa'},
  {id:'W',  l:'W',     s:'param',  x:30, y:60, w:70,h:34,c:'#a78bfa'},
  {id:'b',  l:'b',     s:'param',  x:30, y:260,w:70,h:34,c:'#a78bfa'},
  {id:'mm', l:'x @ W', s:'MmBwd',  x:150,y:110,w:88,h:38,c:'#1e3a6a'},
  {id:'add',l:'+ b',   s:'AddBwd', x:260,y:160,w:70,h:38,c:'#1e3a6a'},
  {id:'rel',l:'ReLU',  s:'ReluBwd',x:260,y:60, w:70,h:38,c:'#1e3a6a'},
  {id:'los',l:'Loss',  s:'MSEBwd', x:350,y:110,w:70,h:38,c:'#6a1e1e'},
];
const E = [
  {f:'x',to:'mm'},{f:'W',to:'mm'},{f:'mm',to:'add'},
  {f:'b',to:'add'},{f:'add',to:'rel'},{f:'rel',to:'los'}
];

function nc(id){const n=N.find(n=>n.id===id);return{x:n.x+n.w/2,y:n.y+n.h/2};}

let svg=`<defs>
<marker id="af" markerWidth="7" markerHeight="5" refX="7" refY="2.5" orient="auto"><polygon points="0 0,7 2.5,0 5" fill="#6a9ffa" opacity="0.5"/></marker>
<marker id="ab" markerWidth="7" markerHeight="5" refX="7" refY="2.5" orient="auto"><polygon points="0 0,7 2.5,0 5" fill="#f0945a" opacity="0.7"/></marker>
</defs>`;
E.forEach((_,i)=>{
  const a=nc(_.f),b=nc(_.to),tn=N.find(n=>n.id===_.to);
  const dx=b.x-a.x,dy=b.y-a.y,l=Math.sqrt(dx*dx+dy*dy);
  const ex=b.x-(dx/l)*(tn.w/2+3),ey=b.y-(dy/l)*(tn.h/2+3);
  svg+=`<line id="ef${i}" x1="${a.x}" y1="${a.y}" x2="${ex}" y2="${ey}" stroke="#6a9ffa" stroke-width="1.5" fill="none" marker-end="url(#af)" opacity="0"/>`;
  const ra=nc(_.to),rb=nc(_.f),fn=N.find(n=>n.id===_.f);
  const rdx=rb.x-ra.x,rdy=rb.y-ra.y,rl=Math.sqrt(rdx*rdx+rdy*rdy);
  const rex=rb.x-(rdx/rl)*(fn.w/2+3),rey=rb.y-(rdy/rl)*(fn.h/2+3);
  svg+=`<line id="eb${i}" x1="${ra.x}" y1="${ra.y}" x2="${rex}" y2="${rey}" stroke="#f0945a" stroke-width="1.5" fill="none" marker-end="url(#ab)" stroke-dasharray="5 3" opacity="0"/>`;
});
N.forEach(n=>{
  svg+=`<g id="n${n.id}" opacity="0.25">
    <rect x="${n.x}" y="${n.y}" width="${n.w}" height="${n.h}" rx="8" fill="${n.c}" stroke="${n.c}" stroke-width="0.5"/>
    <text x="${n.x+n.w/2}" y="${n.y+n.h/2-4}" text-anchor="middle" dominant-baseline="central" fill="#fff" font-family="JetBrains Mono,monospace" font-size="10">${n.l}</text>
    <text x="${n.x+n.w/2}" y="${n.y+n.h/2+9}" text-anchor="middle" dominant-baseline="central" fill="rgba(255,255,255,0.4)" font-family="sans-serif" font-size="7.5">${n.s}</text>
  </g>`;
});
sg.innerHTML=svg;

const S=[
  {a:['x','W','b'],e:[],b:[],h:`<h5>Initial State</h5>
<div class="sr"><span class="nm">x</span><span class="vl">[0.5, -1.2, 0.8]</span><span class="tg p">input</span></div>
<div class="sr"><span class="nm">W</span><span class="vl">3√ó2 matrix</span><span class="tg p">param</span></div>
<div class="sr"><span class="nm">b</span><span class="vl">[0.1, -0.3]</span><span class="tg p">param</span></div>
<h5>Saved for Backward</h5><p class="sim-note">Nothing yet.</p>
<h5>Gradients (.grad)</h5><p class="sim-note">All None.</p>`},

  {a:['x','W','b','mm'],e:[0,1],b:[],h:`<h5>Forward: z‚ÇÅ = x @ W</h5>
<div class="sr"><span class="nm">z‚ÇÅ</span><span class="vl">[0.34, -0.71]</span></div>
<div class="sr"><span class="nm">z‚ÇÅ.grad_fn</span><span class="vl">MmBackward</span></div>
<h5>Saved for Backward</h5>
<div class="sr sv"><span class="nm">MmBwd</span><span class="vl">‚Üí x, W</span><span class="tg s">saved</span></div>
<p class="sim-note hl">Need x for ‚àÇL/‚àÇW = x·µÄ @ grad<br>Need W for ‚àÇL/‚àÇx = grad @ W·µÄ</p>
<h5>Gradients (.grad)</h5><p class="sim-note">All None.</p>`},

  {a:['x','W','b','mm','add'],e:[0,1,2,3],b:[],h:`<h5>Forward: z‚ÇÇ = z‚ÇÅ + b</h5>
<div class="sr"><span class="nm">z‚ÇÇ</span><span class="vl">[0.44, -1.01]</span></div>
<h5>Saved for Backward</h5>
<div class="sr sv"><span class="nm">MmBwd</span><span class="vl">‚Üí x, W</span><span class="tg s">saved</span></div>
<div class="sr"><span class="nm">AddBwd</span><span class="vl">‚Üí (nothing)</span></div>
<p class="sim-note">Add grad is 1 ‚Äî no saves needed.</p>
<h5>Gradients (.grad)</h5><p class="sim-note">All None.</p>`},

  {a:['x','W','b','mm','add','rel'],e:[0,1,2,3,4],b:[],h:`<h5>Forward: y = relu(z‚ÇÇ)</h5>
<div class="sr"><span class="nm">y</span><span class="vl">[0.44, 0.0]</span></div>
<h5>Saved for Backward</h5>
<div class="sr sv"><span class="nm">MmBwd</span><span class="vl">‚Üí x, W</span><span class="tg s">saved</span></div>
<div class="sr"><span class="nm">AddBwd</span><span class="vl">‚Üí (nothing)</span></div>
<div class="sr sv"><span class="nm">ReluBwd</span><span class="vl">‚Üí z‚ÇÇ</span><span class="tg s">saved</span></div>
<p class="sim-note hl">Need z‚ÇÇ to build mask: [True, False]</p>
<h5>Gradients (.grad)</h5><p class="sim-note rd">STILL None. Forward done, no backward yet.</p>`},

  {a:['x','W','b','mm','add','rel','los'],e:[0,1,2,3,4,5],b:[],h:`<h5>Forward: L = MSE(y, target)</h5>
<div class="sr"><span class="nm">L</span><span class="vl">0.2836</span></div>
<h5>Saved for Backward (full graph)</h5>
<div class="sr sv"><span class="nm">MSEBwd</span><span class="vl">‚Üí y, target</span><span class="tg s">saved</span></div>
<div class="sr sv"><span class="nm">ReluBwd</span><span class="vl">‚Üí z‚ÇÇ</span><span class="tg s">saved</span></div>
<div class="sr"><span class="nm">AddBwd</span><span class="vl">‚Üí (nothing)</span></div>
<div class="sr sv"><span class="nm">MmBwd</span><span class="vl">‚Üí x, W</span><span class="tg s">saved</span></div>
<h5>Gradients (.grad)</h5><p class="sim-note rd">STILL ALL NONE. Ready for backward.</p>`},

  {a:['x','W','b','mm','add','rel','los'],e:[],b:[0,1,2,3,4,5],h:`<h5>backward() ‚Äî Gradients computed NOW</h5>
<p class="sim-note" style="color:var(--orange);margin-bottom:0.5rem;">Engine toposorts graph, walks in reverse:</p>
<div class="sr gd"><span class="nm">MSEBwd</span><span class="vl gv">‚àÇL/‚àÇy = 2(y‚àít)/n</span><span class="tg g">uses y,t</span></div>
<div class="sr gd"><span class="nm">ReluBwd</span><span class="vl gv">grad ‚äô (z‚ÇÇ>0)</span><span class="tg g">uses z‚ÇÇ</span></div>
<div class="sr"><span class="nm">AddBwd</span><span class="vl" style="color:var(--muted)">pass through</span></div>
<div class="sr gd"><span class="nm">MmBwd</span><span class="vl gv">x·µÄ@g, g@W·µÄ</span><span class="tg g">uses x,W</span></div>
<h5>Gradients (.grad) ‚Äî NOW computed!</h5>
<div class="sr gd"><span class="nm">W.grad</span><span class="vl gv">[[-0.12, 0.0], ...]</span><span class="tg g">‚úì</span></div>
<div class="sr gd"><span class="nm">b.grad</span><span class="vl gv">[0.44, 0.0]</span><span class="tg g">‚úì</span></div>`}
];

function go(s){
  const d=S[s];
  document.querySelectorAll('.sbtn').forEach((b,i)=>b.classList.toggle('on',i===s));
  N.forEach(n=>{document.getElementById('n'+n.id).setAttribute('opacity',d.a.includes(n.id)?'1':'0.2');});
  E.forEach((_,i)=>{
    document.getElementById('ef'+i).setAttribute('opacity',d.e.includes(i)?'0.5':'0');
    const eb=document.getElementById('eb'+i);
    if(d.b.includes(i)){eb.setAttribute('opacity','0.7');eb.classList.add('flowing');}
    else{eb.setAttribute('opacity','0');eb.classList.remove('flowing');}
  });
  sp.innerHTML=d.h;
}
document.querySelectorAll('.sbtn').forEach(b=>b.addEventListener('click',()=>go(+b.dataset.s)));
</script>
</body>
</html>
